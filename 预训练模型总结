BERT：
BERT使用了Transformer编码器结构进行对大量未标注的语料进行无监督的预训练；
训练方式主要有两种，一种是随机mask一些单词，然后对这些单词进行还原；另一种是下一个句子预测的分类任务，判断这两个句子是不是连续的两句话。

Roberta：
Roberta相对于BERT的改进主要有两点：
首先相对于BERT的mask机制是在预处理阶段进行的（静态的mask机制），Roberta使用了动态的mask机制，在训练的时候随机mask输入中15%的单词；
然后Roberta省略了BERT中的下一句子预测任务，因为作者发现这个任务并没有什么用处，好像还会损害性能。

ALBert：
ALbert的主要贡献就是大大减小了BERT的占用空间，主要体现在两个方面：
首先ALbert对词向量进行了分解，把词向量分解成了两个矩阵的乘积；
然后ALbert还对不同层的参数进行了共享，这样也可以大大减少了参数的数量。
https://www.zhihu.com/question/347898375/answer/863537122

Transformer-XL：
Transformer-XL主要是为了解决BERT中的长文本截断问题，因为BERT的训练时只能处理512长度的输入，对于长文本进行了截断；
Transformer-XL通过缓存之前的文本表示，使得可以看到上一个片段的语境信息；
但是这样会有一个问题，因为如果不同片段的内容的位置编码都是从1开始编码的话，会发生位置的混乱，所以作者提出了相对位置编码的方法，具体是修改attention中的计算；


XLNet:
XLNet主要是为了解决BERT预训练和微调阶段的不一致问题，因为在预训练阶段会有15%的单词被MASK，而在微调的时候没有MASK的单词；
而且MASK会导致的问题是MASK的单词之间也存在着语义的关系，这样会导致MASK机制的预测会出现问题；
所以这篇文章提出使用mask attention的方式，这样就不会显示的引入mask符号；
而且为了解决预测的时候需要知道当前的位置信息，作者提出了双流attention的机制，一个content attention用来attention其他内容，一个query attention只引入位置信息进行查询；
最终使用query attention的结果进行预训练阶段的预测工作，而在微调其他任务阶段直接将query attention抛弃即可，一个非常优美的结构。


GPT-3:
主要贡献是使用Transformer decoder的结构做少样本的迁移学习，而且迁移学习并不需要进行梯度的更新，因为它会把给出的少样本作为以前的输入来进行少样本下的迁移学习；
而且对于指定任务的学习通过在输入中给出任务名字来实现。

T5:
T5的主要贡献也是在输入中指定任务名字来做对应的任务，而且它把所有的东西都用文本来表示，包括所有的数字等等。
